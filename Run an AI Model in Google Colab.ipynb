{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Running any Local AI Model in Google Colab is simple...\n",
        "\n",
        "1.   click on the dropdown after \"Connect\" and change the runtime to GPU T4\n",
        "2.   then, run the following code\n"
      ],
      "metadata": {
        "id": "p188kuutvcbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install -y cmake g++"
      ],
      "metadata": {
        "id": "f8MxwxQVv_Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch numpy sentencepiece"
      ],
      "metadata": {
        "id": "WbI2zMmlwdhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp.git"
      ],
      "metadata": {
        "id": "brg5xrahwi-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd llama.cpp"
      ],
      "metadata": {
        "id": "LBnM3A9DwnnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir build"
      ],
      "metadata": {
        "id": "62cZhduwwr4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cmake -B build"
      ],
      "metadata": {
        "id": "cukqbbqPwuXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cmake --build build --config Release"
      ],
      "metadata": {
        "id": "Vq2Va3QKwyWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "then go to huggingface.co, search for a model, then select the model and click the black box named \"Use this model\" and after the dialog appears click llama.cpp and then click \"Use pre-built binary\" and copy the second code block, for this example, we will use an uncensord version of Llama 3.2B"
      ],
      "metadata": {
        "id": "T355BU6-w41j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and run the model:\n",
        "./llama-cli \\\n",
        "  --hf-repo \"Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF\" \\\n",
        "  --hf-file Llama-3.1-8B-Lexi-Uncensored_V2_F16.gguf \\\n",
        "  -p \"You are a helpful assistant\" \\\n",
        "  --conversation"
      ],
      "metadata": {
        "id": "tBw2iWkD0Ofk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}